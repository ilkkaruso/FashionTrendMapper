---
phase: 02-data-collection
plan: 05
type: execute
wave: 3
depends_on: ["02-04"]
files_modified:
  - app/api/cron/fetch-trends/route.ts
autonomous: true
user_setup:
  - service: vercel-cron
    why: "Cron authentication for daily job"
    env_vars:
      - name: CRON_SECRET
        source: "Generate random string: openssl rand -base64 32"
    dashboard_config:
      - task: "Add CRON_SECRET to Vercel environment variables"
        location: "Vercel Dashboard -> Project -> Settings -> Environment Variables"

must_haves:
  truths:
    - "Cron endpoint fetches trends from both sources"
    - "Cron endpoint normalizes and aggregates scores"
    - "Cron endpoint saves trends to database"
    - "Cron endpoint requires authentication"
    - "Cron job configured for daily 5am UTC"
  artifacts:
    - path: "app/api/cron/fetch-trends/route.ts"
      provides: "Daily cron job endpoint"
      exports: ["GET"]
  key_links:
    - from: "app/api/cron/fetch-trends/route.ts"
      to: "lib/fetchers/google-trends.ts"
      via: "fetchGoogleTrends()"
      pattern: "fetchGoogleTrends"
    - from: "app/api/cron/fetch-trends/route.ts"
      to: "lib/fetchers/reddit-trends.ts"
      via: "fetchRedditTrends()"
      pattern: "fetchRedditTrends"
    - from: "app/api/cron/fetch-trends/route.ts"
      to: "lib/normalizers/score-normalizer.ts"
      via: "normalizeScores(), aggregateTrends()"
      pattern: "normalizeScores|aggregateTrends"
    - from: "app/api/cron/fetch-trends/route.ts"
      to: "lib/database/trend-repository.ts"
      via: "saveTrendsWithHistory()"
      pattern: "saveTrendsWithHistory"
---

<objective>
Wire all fetchers, normalizers, and database into the cron endpoint.

Purpose: Create the main cron job endpoint that Vercel calls daily at 5am UTC. This orchestrates the entire data collection pipeline.
Output: Working cron endpoint that fetches, normalizes, and persists fashion trends.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-data-collection/02-RESEARCH.md
@vercel.json
@lib/fetchers/google-trends.ts
@lib/fetchers/reddit-trends.ts
@lib/normalizers/score-normalizer.ts
@lib/database/trend-repository.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create cron endpoint</name>
  <files>app/api/cron/fetch-trends/route.ts</files>
  <action>
Create `app/api/cron/fetch-trends/route.ts`:

```typescript
import { NextRequest } from 'next/server';
import { fetchGoogleTrends } from '@/lib/fetchers/google-trends';
import { fetchRedditTrends } from '@/lib/fetchers/reddit-trends';
import { normalizeScores, aggregateTrends } from '@/lib/normalizers/score-normalizer';
import { saveTrendsWithHistory } from '@/lib/database/trend-repository';

// CRITICAL: Prevent response caching
export const dynamic = 'force-dynamic';

// Extend timeout for Vercel Pro (optional, defaults to 10s on Hobby)
export const maxDuration = 60;

export async function GET(request: NextRequest) {
  const startTime = Date.now();

  // 1. Authenticate - Vercel sends CRON_SECRET in Authorization header
  const authHeader = request.headers.get('authorization');
  if (authHeader !== `Bearer ${process.env.CRON_SECRET}`) {
    console.error('Cron auth failed - invalid or missing CRON_SECRET');
    return Response.json(
      { error: 'Unauthorized', hint: 'CRON_SECRET mismatch' },
      { status: 401 }
    );
  }

  console.log('Cron job started:', new Date().toISOString());

  try {
    // 2. Fetch from both sources in parallel (critical for 10s timeout)
    const [googleResult, redditResult] = await Promise.all([
      fetchGoogleTrends(),
      fetchRedditTrends(),
    ]);

    console.log(`Google: ${googleResult.success ? googleResult.data?.length : 'failed'} trends`);
    console.log(`Reddit: ${redditResult.success ? redditResult.data?.length : 'failed'} trends`);

    // 3. Combine raw trends (handle partial failures)
    const rawTrends = [
      ...(googleResult.data || []),
      ...(redditResult.data || []),
    ];

    if (rawTrends.length === 0) {
      console.error('No trends fetched from any source');
      return Response.json({
        success: false,
        error: 'No data from any source',
        googleError: googleResult.error,
        redditError: redditResult.error,
        duration: Date.now() - startTime,
      }, { status: 500 });
    }

    // 4. Normalize scores to 0-100 scale
    const normalized = normalizeScores(rawTrends);
    console.log(`Normalized: ${normalized.length} trends`);

    // 5. Aggregate by title (merge cross-source trends)
    const aggregated = aggregateTrends(normalized);
    console.log(`Aggregated: ${aggregated.length} unique trends`);

    // 6. Save to database with history
    await saveTrendsWithHistory(aggregated);
    console.log('Saved to database');

    const duration = Date.now() - startTime;
    console.log(`Cron job completed in ${duration}ms`);

    return Response.json({
      success: true,
      stats: {
        google: {
          success: googleResult.success,
          count: googleResult.data?.length || 0,
          cached: googleResult.cached || false,
        },
        reddit: {
          success: redditResult.success,
          count: redditResult.data?.length || 0,
          cached: redditResult.cached || false,
        },
        normalized: normalized.length,
        aggregated: aggregated.length,
        duration,
      },
      timestamp: new Date().toISOString(),
    });

  } catch (error) {
    const duration = Date.now() - startTime;
    console.error('Cron job failed:', error);

    return Response.json({
      success: false,
      error: error instanceof Error ? error.message : 'Unknown error',
      duration,
      timestamp: new Date().toISOString(),
    }, { status: 500 });
  }
}
```

Key implementation details:
- `dynamic = 'force-dynamic'` prevents response caching
- `maxDuration = 60` extends timeout on Pro plan (ignored on Hobby)
- Promise.all for parallel fetching (critical for 10s timeout)
- Handles partial failures (one source down, other works)
- Detailed logging for debugging in Vercel logs
- Returns comprehensive stats for monitoring
  </action>
  <verify>
`npx tsc --noEmit` passes.
`npm run build` succeeds.
Local test (with valid CRON_SECRET env var):
  `curl -H "Authorization: Bearer $CRON_SECRET" http://localhost:3000/api/cron/fetch-trends`
  Returns JSON with success/stats.
  </verify>
  <done>
Cron endpoint at /api/cron/fetch-trends:
- Authenticates with CRON_SECRET
- Fetches Google + Reddit in parallel
- Normalizes and aggregates scores
- Saves to database with history
- Returns detailed stats
- Handles partial failures gracefully
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify end-to-end pipeline</name>
  <files>None (verification only)</files>
  <action>
End-to-end verification steps:

1. Ensure all env vars are set locally (.env.local):
   - NEXT_PUBLIC_SUPABASE_URL
   - NEXT_PUBLIC_SUPABASE_PUBLISHABLE_KEY
   - UPSTASH_REDIS_REST_URL
   - UPSTASH_REDIS_REST_TOKEN
   - REDDIT_CLIENT_ID
   - REDDIT_CLIENT_SECRET
   - REDDIT_REFRESH_TOKEN
   - REDDIT_USER_AGENT
   - CRON_SECRET

2. Start dev server: `npm run dev`

3. Test individual fetchers:
   - `curl http://localhost:3000/api/test/google-trends`
   - `curl http://localhost:3000/api/test/reddit-trends`

4. Test full cron endpoint:
   ```bash
   curl -H "Authorization: Bearer YOUR_CRON_SECRET" \
        http://localhost:3000/api/cron/fetch-trends
   ```

5. Verify database has data:
   - Check Supabase dashboard: trends table should have rows
   - Check trend_sources table for per-source scores
   - Check trend_history table for today's snapshot

6. Test auth rejection:
   ```bash
   curl http://localhost:3000/api/cron/fetch-trends
   # Should return 401 Unauthorized
   ```

Document any issues or missing env vars in SUMMARY.
  </action>
  <verify>
All tests pass:
- Test endpoints return data
- Cron endpoint returns success
- Database has trends, sources, history
- Unauthorized requests rejected
  </verify>
  <done>
Full pipeline verified:
- Fetchers retrieve data from Google/Reddit
- Normalizer scales scores to 0-100
- Repository saves to database
- History tracking works
- Auth protects cron endpoint
  </done>
</task>

</tasks>

<verification>
1. Build succeeds: `npm run build`
2. vercel.json has correct cron path: /api/cron/fetch-trends
3. Cron endpoint responds to authenticated requests
4. Database shows trends after cron run
5. Unauthenticated requests return 401
</verification>

<success_criteria>
- [ ] app/api/cron/fetch-trends/route.ts created
- [ ] Endpoint authenticates with CRON_SECRET
- [ ] Fetches from Google and Reddit in parallel
- [ ] Normalizes scores using normalizeScores()
- [ ] Aggregates by title using aggregateTrends()
- [ ] Saves to database using saveTrendsWithHistory()
- [ ] Returns detailed stats (counts, duration, success flags)
- [ ] Handles partial failures (one source down)
- [ ] Build passes
- [ ] vercel.json cron path matches endpoint path exactly
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-collection/02-05-SUMMARY.md`
</output>
